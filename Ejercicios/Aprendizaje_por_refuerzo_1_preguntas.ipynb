{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ibwGb5Boj4f"
      },
      "source": [
        "# Ejercicio 1: Problema de Bandidos\n",
        "\n",
        "En esta seccion estudiaremos las diferentes manera de seleccionar acciones que fueron vistas en clases:\n",
        "\n",
        "- Seleccion golosa\n",
        "- $\\epsilon$-greedy\n",
        "- softmax\n",
        "\n",
        "Utilizaremos la siguiente clase que permite simular un problema de bandidos\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MknTK79jodxy"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "rng = np.random.default_rng()\n",
        "\n",
        "class Bandit:\n",
        "    \"\"\"\n",
        "    n-armed bandit.\n",
        "    \"\"\"\n",
        "    def __init__(self, nb_actions, mean=0.0, std_Q=1.0, std_r=1.0):\n",
        "        \"\"\"\n",
        "        :param nb_actions: numbero de acciones\n",
        "        :param mean: promedio de la distribucion normal desde la cual se obtiene $Q^*$.\n",
        "        :param std_Q: desviación estándar de $Q^*$.\n",
        "        :param std_r: desviación estándar de las recompensas muestreadas.\n",
        "        \"\"\"\n",
        "        # Almacenar parámetros\n",
        "        self.nb_actions = nb_actions\n",
        "        self.mean = mean\n",
        "        self.std_Q = std_Q\n",
        "        self.std_r = std_r\n",
        "\n",
        "        # Inicializar los valores de Q reales (desconocidos para el agente)\n",
        "        self.Q_star = rng.normal(self.mean, self.std_Q, self.nb_actions)\n",
        "\n",
        "        # Acción óptima\n",
        "        self.a_star = self.Q_star.argmax()\n",
        "\n",
        "    def step(self, action):\n",
        "        \"\"\"\n",
        "        Obtener una muestra del bandido\n",
        "\n",
        "        :param action: la acción seleccionada.\n",
        "        :return: la recompensa.\n",
        "        \"\"\"\n",
        "        return float(rng.normal(self.Q_star[action], self.std_r, 1))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U4LwbqMLHXQA"
      },
      "source": [
        "**Pregunta 1:** Cree un bandido con 5 acciones y por cada acción obtenga 10 muestras. Luego grafique el promedio de recompensa de cada acción y compare con los valores reales. Aumente la cantidad de muestras a 1000 y vuelva a realizar la comparación."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "Sy_GkSDyINi2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Harunomi\\AppData\\Local\\Temp\\ipykernel_10404\\3628562260.py:37: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
            "  return float(rng.normal(self.Q_star[action], self.std_r, 1))\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<BarContainer object of 5 artists>"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "rng = np.random.default_rng()\n",
        "\n",
        "class Bandit:\n",
        "    \"\"\"\n",
        "    n-armed bandit.\n",
        "    \"\"\"\n",
        "    def __init__(self, nb_actions, mean=0.0, std_Q=1.0, std_r=1.0):\n",
        "        \"\"\"\n",
        "        :param nb_actions: numbero de acciones\n",
        "        :param mean: promedio de la distribucion normal desde la cual se obtiene $Q^*$.\n",
        "        :param std_Q: desviación estándar de $Q^*$.\n",
        "        :param std_r: desviación estándar de las recompensas muestreadas.\n",
        "        \"\"\"\n",
        "        # Almacenar parámetros\n",
        "        self.nb_actions = nb_actions\n",
        "        self.mean = mean\n",
        "        self.std_Q = std_Q\n",
        "        self.std_r = std_r\n",
        "\n",
        "        # Inicializar los valores de Q reales (desconocidos para el agente)\n",
        "        self.Q_star = rng.normal(self.mean, self.std_Q, self.nb_actions)\n",
        "\n",
        "        # Acción óptima\n",
        "        self.a_star = self.Q_star.argmax()\n",
        "\n",
        "    def step(self, action):\n",
        "        \"\"\"\n",
        "        Obtener una muestra del bandido\n",
        "\n",
        "        :param action: la acción seleccionada.\n",
        "        :return: la recompensa.\n",
        "        \"\"\"\n",
        "        return float(rng.normal(self.Q_star[action], self.std_r, 1))\n",
        "\n",
        "n_actions = 5\n",
        "bandit = Bandit( n_actions, 0.3, 0.5, 0.34)\n",
        "Q_reward = []\n",
        "muestras = 1000\n",
        "for i in range(n_actions):\n",
        "    results = []\n",
        "    for j in range(muestras):\n",
        "        results.append(bandit.step(i))\n",
        "    Q_reward.append(np.mean(results))\n",
        "\n",
        "x = [i for i in range(1,n_actions+1)]\n",
        "\n",
        "plt.bar(x,Q_reward)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s5XCabeeIq9G"
      },
      "source": [
        "En la seleccion golosa, siempre escogemos la acción con el valor máximo de Q (o aleatoriamente si hay un empate):\n",
        "\n",
        "$a_t = argmax_a Q(a)$\n",
        "\n",
        "Podemos mantener estimaciones de los valores de Q de la siguiente forma:\n",
        "\n",
        "$Q_{t+1}(a_t) = Q_t + \\alpha(r_t - Q_t(a_t))$\n",
        "\n",
        "Estos valores deben ser actualizados después de ejecutar cada acción $a_t$ y recibir la recompensa $r_t$.\n",
        "\n",
        "**Pregunta 2:** Implemente la seleccion golosa de acciones.\n",
        "\n",
        "El algoritmo debe\n",
        "\n",
        "1. Crear un bandido de 5 brazos (media 0 y varianza 1)\n",
        "2. Crear un arreglo para las estimaciones de Q e iniciar los valores en 0.\n",
        "3. Simular 100 jugadas en las que deberá:\n",
        "- Seleccionar una acción de manera goloza usando las estimaciones actuales\n",
        "- Obtener una muestra usando la acción seleccionada\n",
        "- Actualizar los valores de Q de la acción seleccionada\n",
        "\n",
        "Sugerencia: Para implementar argmax no utilizar np.argmax() ya que si hay algún empate x.argmax() siempre retorna el índice de la primera ocurrencia. Para implementar una acción aleatoria en caso de empate:\n",
        "\n",
        "a = rng.choice(np.where(x == x.max())[0])\n",
        "\n",
        "Almacene y cree un gráfico con las recompensas recibidas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VyoQSK3oJODX"
      },
      "outputs": [],
      "source": [
        "# Escriba aqui su código"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yjWFNZPSJcEd"
      },
      "source": [
        "**Pregunta 3:** Vuelva a correr su algoritmo con multiples valores diferentes para Q* (simplemente cree un nuevo Bandido) y observe\n",
        "\n",
        "- Cuanta recompensa obtiene\n",
        "- Cuanto difieren los valores estimados de Q de los reales.\n",
        "- Si la seleccion golosa converge en la acción óptima."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rYCnWkJ-J585"
      },
      "source": [
        "**Pregunta 4:** Cree una clase que implemente un agente goloso GreedyAgent que tome el bandido y la tasa de aprendizaje como argumento  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pLHyWk-NKFJJ"
      },
      "outputs": [],
      "source": [
        "class GreedyAgent:\n",
        "\n",
        "    def __init__(self, bandit, alpha):\n",
        "\n",
        "        self.bandit = bandit\n",
        "        self.alpha = alpha\n",
        "\n",
        "        # Estimated Q-values\n",
        "        self.Q_t = np.zeros(self.bandit.nb_actions)\n",
        "\n",
        "    def act(self):\n",
        "\n",
        "        action = #...\n",
        "        return action\n",
        "\n",
        "    def update(self, action, reward):\n",
        "\n",
        "        self.Q_t[action] += #...\n",
        "\n",
        "\n",
        "    def train(self, nb_steps):\n",
        "\n",
        "        rewards = []\n",
        "\n",
        "        for step in range(nb_steps):\n",
        "\n",
        "            # Seleccionar la acción\n",
        "            action = self.act()\n",
        "\n",
        "            # Obtener la muestra de recompensa\n",
        "            reward = self.bandit.step(action)\n",
        "\n",
        "            # Almacenar la recompensa\n",
        "            rewards.append(reward)\n",
        "\n",
        "            # Actualizar los valores Q\n",
        "            self.update(action, reward)\n",
        "\n",
        "        return np.array(rewards)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3JaTWC4ZKRP9"
      },
      "source": [
        "**Pregunta 5:** Modifique el método train() para que retorne una lista de valores binatios que indiquen si el agente escogio la acción óptima. Ejecute una simulación como las anteriores y observe cuantas veces el agente escogio la acción óptima."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AB2S3860KTx6"
      },
      "outputs": [],
      "source": [
        "#Escriba aqui su código"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WPiD3342Kl35"
      },
      "source": [
        "**Pregunta 6:** Corra el proceso de aprendizaje 200 veces (nuevos bandidos y agente en cada proceso) y promedie los resultados. Dele un nombre único a cada arreglo ya que los usaremos después (por ejemplo rewards_greedy o optimal_greedy). Compare los resultados con lo visto en clases."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RXN81HnJwLmV"
      },
      "outputs": [],
      "source": [
        "#Escriba aqui su código"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O97jybV_LIB-"
      },
      "source": [
        "# Seleccion ϵ-greedy\n",
        "\n",
        "La seleccion golosa no explora: en cuanto encuentre una acción con un valor positivo seguirá escogiendo siempre la misma acción. El valor estimado de la acción escogida sera correcto pero todas las otras acciones se mantendrán en 0.\n",
        "\n",
        "En la selección ϵ-greedy, acciones con menores valores son escogidas con un probabilidad pequeña:\n",
        "\n",
        "$\\pi(a) = \\begin{cases} 1-\\epsilon & \\text{si a=a*}\\\\ \\frac{\\epsilon}{|A-1|} & \\text{en otro caso} \\end{cases}$\n",
        "\n",
        "**Pregunta 7:** Cree un agente ϵ-greedy y repita el experimento anterior. No borre los arreglos anteriores para así poder comparar ambos métodos en un mismo gráfico.\n",
        "\n",
        "El agente debe:\n",
        "\n",
        "- Determinar la acción golosa​\n",
        "- Obtener un número aleatorio entre 0 y 1 (rng.random()).\n",
        "Si este número es menor que ϵ entonces seleccionar otra accion aleatoria entre las restantes (rng.choice()).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xf7IlpPeyPt4"
      },
      "outputs": [],
      "source": [
        "class EpsilonGreedyAgent(GreedyAgent):\n",
        "\n",
        "    def __init__(self, bandit, alpha, epsilon):\n",
        "\n",
        "        self.epsilon = epsilon\n",
        "\n",
        "        self.actions = np.arange(bandit.nb_actions)\n",
        "\n",
        "        super().__init__(bandit, alpha)\n",
        "\n",
        "    def act(self):\n",
        "\n",
        "        action = #...\n",
        "\n",
        "        return action\n",
        "\n",
        "nb_actions = 5\n",
        "\n",
        "# Tasa de aprendizaje\n",
        "alpha = 0.1\n",
        "\n",
        "# Epsilon para la exploracion\n",
        "epsilon = 0.1\n",
        "\n",
        "rewards_egreedy = []\n",
        "optimal_egreedy = []\n",
        "\n",
        "for trial in range(200):\n",
        "\n",
        "    bandit = Bandit(nb_actions)\n",
        "\n",
        "    agent = EpsilonGreedyAgent(bandit, alpha, epsilon)\n",
        "\n",
        "    rewards, optimal = agent.train(1000)\n",
        "\n",
        "    rewards_egreedy.append(rewards)\n",
        "    optimal_egreedy.append(optimal)\n",
        "\n",
        "rewards_egreedy = np.mean(rewards_egreedy, axis=0)\n",
        "optimal_egreedy = np.mean(optimal_egreedy, axis=0)\n",
        "\n",
        "# Plot the Q-values and the evolution of rewards\n",
        "plt.figure(figsize=(12, 5))\n",
        "plt.subplot(121)\n",
        "plt.plot(rewards_greedy, label=\"Greedy\")\n",
        "plt.plot(rewards_egreedy, label=\"$\\epsilon$-Greedy\")\n",
        "plt.legend()\n",
        "plt.xlabel(\"Plays\")\n",
        "plt.ylabel(\"Reward\")\n",
        "plt.subplot(122)\n",
        "plt.plot(optimal_greedy, label=\"Greedy\")\n",
        "plt.plot(optimal_egreedy, label=\"$\\epsilon$-Greedy\")\n",
        "plt.legend()\n",
        "plt.xlabel(\"Plays\")\n",
        "plt.ylabel(\"Optimal\")\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xfZ6S79OMG89"
      },
      "source": [
        "# Seleccion softmax\n",
        "\n",
        "En esta estrategia los valores de Q son transformados en una distribución de las probabilidades sobre las acciones:\n",
        "\n",
        "$\\pi(s) = \\frac{exp Q(a)/\\tau}{\\sum_b exp Q(b)/\\tau}$\n",
        "\n",
        "En la práctica para valores muy grandes de Q o si la temperatura es muy pequeña los valores de $expQ(a)/\\tau$ pueden crear problemas numéricos. Para evitar esto usamos la siguiente fórmula:\n",
        "\n",
        "$\\pi(s) = \\frac{exp \\frac{Q_t(a)-max_aQ_t(a)}{\\tau}}{\\sum_b  \\frac{Q_t(b)-max_aQ_t(a)}{\\tau}}$\n",
        "\n",
        "$Q_t(a)-max_aQ_t(a)$ es siempre negativo por lo que el exponencial siempre esta entre 0 y 1.\n",
        "\n",
        "**Pregunta 8:** Implemente un agente con selección softmax (con $\\tau=0.5$) y compare con el agente goloso y el $\\epsilon$-greedy. Cambie la temperatura y determine el mejor valor. Concluya.\n",
        "\n",
        "Sugerencia: para escoger acciones usar action = rng.choice(self.actions, p=proba_softmax)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eUD5-FLKMfec"
      },
      "outputs": [],
      "source": [
        "class SoftmaxAgent(GreedyAgent):\n",
        "\n",
        "    def __init__(self, bandit, alpha, tau):\n",
        "        self.tau = tau\n",
        "\n",
        "        # List of actions\n",
        "        self.actions = np.arange(bandit.nb_actions)\n",
        "\n",
        "        # Call the constructor of GreedyAgent\n",
        "        super().__init__(bandit, alpha)\n",
        "\n",
        "    def act(self):\n",
        "\n",
        "        action = #...\n",
        "        return action\n",
        "\n",
        "nb_actions = 5\n",
        "\n",
        "alpha = 0.1\n",
        "\n",
        "tau = 0.1\n",
        "\n",
        "rewards_softmax = []\n",
        "optimal_softmax = []\n",
        "\n",
        "for trial in range(200):\n",
        "\n",
        "    bandit = Bandit(nb_actions)\n",
        "\n",
        "    agent = SoftmaxAgent(bandit, alpha, tau)\n",
        "\n",
        "    rewards, optimal = agent.train(1000)\n",
        "\n",
        "    rewards_softmax.append(rewards)\n",
        "    optimal_softmax.append(optimal)\n",
        "\n",
        "rewards_softmax = np.mean(rewards_softmax, axis=0)\n",
        "optimal_softmax = np.mean(optimal_softmax, axis=0)\n",
        "\n",
        "plt.figure(figsize=(12, 5))\n",
        "plt.subplot(121)\n",
        "plt.plot(rewards_greedy, label=\"Greedy\")\n",
        "plt.plot(rewards_egreedy, label=\"$\\epsilon$-Greedy\")\n",
        "plt.plot(rewards_softmax, label=\"Softmax\")\n",
        "plt.legend()\n",
        "plt.xlabel(\"Plays\")\n",
        "plt.ylabel(\"Reward\")\n",
        "plt.subplot(122)\n",
        "plt.plot(optimal_greedy, label=\"Greedy\")\n",
        "plt.plot(optimal_egreedy, label=\"$\\epsilon$-Greedy\")\n",
        "plt.plot(optimal_softmax, label=\"Softmax\")\n",
        "plt.legend()\n",
        "plt.xlabel(\"Plays\")\n",
        "plt.ylabel(\"Optimal\")\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
