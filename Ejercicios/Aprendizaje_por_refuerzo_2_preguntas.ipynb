{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "uso0_DAPnU3f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ejercicio 4: Programación Dinámica\n",
        "\n",
        "Un robot reciclador tiene que buscar latas que recoger (cada lata define una recompensa para el robot). El robot puede decidir quedarse donde está para guardar batería y esperar que alguien traiga una lata (esto le entrega menos recompensa en promedio).\n",
        "\n",
        "El robot tiene dos niveles de batería, alto y bajo.\n",
        "\n",
        "En el nivel alto el robot puede buscar o esperar.\n",
        "\n",
        "En el estado bajo el robot puede buscar, esperar o recargar.\n",
        "\n",
        "Las transiciones estado-acción son probabilísticas y dependen de $\\alpha$ y $\\beta$.\n",
        "\n",
        "El problema puede verse como un proceso de decisiones de Markov con dos estados alto y bajo correspondiente al nivel de batería.\n",
        "\n",
        "$\\mathcal{S}$ = {alto , bajo}\n",
        "\n",
        "$\\mathcal{A}(alto)$ = {buscar, esperar}\n",
        "\n",
        "$\\mathcal{A}(bajo)$ = {buscar, esperar, recargar}\n",
        "\n",
        "La acción buscar tiene como promedio de recompensa $\\mathcal{R}^\\text{search}$,  la acción esperar tiene como promedio de recompensa  $\\mathcal{R}^\\text{wait}$, la acción recargar no trae recompensa pero permite llegar al estado alto.\n",
        "\n",
        "Si el robot decide buscar en el estado bajo, hay una probabilidad $1 - \\beta$ que su bateria termine vacia requiriendo intervención humana. Esto se castiga con una recompensa de -3.\n",
        "\n",
        "|s\t|s’\t|a\t|p(s’ / s, a)\t|r(s, a, s’)|\n",
        "|:---|:---|:---|:---|:---\n",
        "|alto\t|alto\t|buscar\t|$\\alpha$\t    |$\\mathcal{R}^\\text{buscar}$\n",
        "|alto\t|bajo\t|buscar\t|$1 - \\alpha$\t|$\\mathcal{R}^\\text{buscar}$\n",
        "|bajo\t|alto\t|buscar\t|$1 - \\beta$\t|−3\n",
        "|bajo\t|bajo\t|buscar\t|$\\beta$\t    |$\\mathcal{R}^\\text{buscar}$\n",
        "|alto\t|alto\t|esperar\t  |1\t          |$\\mathcal{R}^\\text{esperar}$\n",
        "|alto\t|bajo\t|esperar\t  |0\t          |$\\mathcal{R}^\\text{esperar}$\n",
        "|bajo\t|alto\t|esperar\t  |0\t          |$\\mathcal{R}^\\text{esperar}$\n",
        "|bajo\t|bajo\t|esperar\t  |1\t          |$\\mathcal{R}^\\text{esperar}$\n",
        "|bajo\t|alto\t|recargar\t|1\t|0\n",
        "|bajo\t|bajo\t|recargar\t|0\t|0\n",
        "\n",
        "\n",
        "El objetivo de este ejercicio es encontrar la estrategia óptima $\\pi^*$, es decir encontrar por cada esta la acción que debe ser ejecutada para obtener mayor recompensa en el largo plazo.\n",
        "\n",
        "Vamos a aplicar dos métodos de programación dinámica: iteración de estrategias e iteración de valores para resolver las ecuaciones de Bellman.\n",
        "\n",
        "La ecuación de Bellman para la funcion de estados:\n",
        "\n",
        "$V^\\pi(s) = \\sum_a \\pi(s,a) \\sum_{s'} p(s'|a,s') [r(s,a,s') + \\gamma V^\\pi(s')] $\n",
        "\n",
        "**Pregunta 1**: En papel adapte las ecuaciones de Bellman al problema. Primero, por cada estado s y posible acción a, encuentre el valor óptimo de cada acción en cada estado:\n",
        "\n",
        "$Q^\\pi (s,a) = f(V^\\pi (alto), V^\\pi (bajo), \\alpha, \\beta, \\gamma, \\mathcal{R}^\\text{buscar},\\mathcal{R}^\\text{esperar} )$\n",
        "\n",
        "Deduzca la ecuación de Bellman para los dos estados $V^\\pi(\\text{alto})$ y $V^\\pi (\\text{bajo})$ que dependa de los valores de Q obtenidos:\n",
        "\n",
        "$V(s)=\\sum_a \\pi(s,a)Q(s,a)$\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "5Mrh7Q27nGdk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Respuesta:**\n",
        "\n",
        "$Q^\\pi (alto,buscar) = ...$\n",
        "\n",
        "$Q^\\pi(alto,esperar) = ...$\n",
        "\n",
        "$Q^\\pi(bajo,buscar) = ...$\n",
        "\n",
        "$Q^\\pi(bajo,esperar) = ...$\n",
        "\n",
        "$Q^\\pi(bajo,recargar) = ...$\n",
        "\n",
        "$V^\\pi(alto) = ...$\n",
        "\n",
        "$V^\\pi(bajo) = ...$\n"
      ],
      "metadata": {
        "id": "ieFiF2TxwWDD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Iteración de estrategias**\n",
        "Podemos resolver las ecuaciones usando evaluación de estrategias iterativa para una estrategia $\\pi$.\n",
        "\n",
        "Primero ajustemos los parámetros del proceso de Markov. En el resto del ejercicio estudiaremos los efectos de estos parámetros."
      ],
      "metadata": {
        "id": "nBFS9QeTzf4W"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z2mwNKO3nFPx"
      },
      "outputs": [],
      "source": [
        "# Transiciones\n",
        "alpha = 0.3\n",
        "beta = 0.2\n",
        "\n",
        "# Descuento\n",
        "gamma = 0.7\n",
        "\n",
        "# Recompensas\n",
        "r_buscar = 6.0\n",
        "r_esperar = 2.0"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Usaremos un diccionario para representar las acciones"
      ],
      "metadata": {
        "id": "cg2GEjHez_Jr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nb_states = 2\n",
        "nb_actions = 3\n",
        "\n",
        "s = {'alto': 0, 'bajo': 1}\n",
        "a = {'buscar': 0, 'esperar': 1, 'recargar': 2}"
      ],
      "metadata": {
        "id": "F7GFlKH40KZF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ahora inicializaremos los arreglos en que almacenaremos V y Q. V tendrá dos elementos, para bajo y alto. Q será una matriz de 2x3 con un elemento por cada par estado-acción."
      ],
      "metadata": {
        "id": "-vPQrfNZ0Osq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "V = np.zeros(nb_states)\n",
        "Q = np.zeros((nb_states, nb_actions))"
      ],
      "metadata": {
        "id": "sEz0CtR60qqi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Puede acceder a los valores individuales con V[s['alto']] or Q[s['bajo'], a['esperar']].\n",
        "\n",
        "Ahora podemos evaluar una estrategia $\\pi$. Para usar programación dinámica usaremos una estrategia deterministica.\n",
        "\n",
        "Para implementar la estrategia solo necesitamos asignar el indice de una acción a cada estado, $\\pi(s)$. A continuación crearemos una estrategia inicial $\\pi$ en la que el agente busca en ambos estados."
      ],
      "metadata": {
        "id": "pzGcsVIi2fC1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pi = np.array([a['buscar'], a['buscar']], dtype=int)\n"
      ],
      "metadata": {
        "id": "yWf6QCPT3f4F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Pregunta 1:** Evaluar esta estrategia usando evaluacion iterativa.\n",
        "\n",
        "$V(s) = \\sum_a \\pi(s,a) \\sum p(s'|s,a) [r(s,a,s') + \\gamma V(s')]$\n",
        "\n",
        "Sugerencia: el código sera mas simple si primero actualiza los valores Q de los 5 pares estado-accion:\n",
        "\n",
        "$Q(s,a) = \\sum p(s'|s,a) [r(s,a,s') + \\gamma V(s')]$\n",
        "\n",
        "$V(s) = \\sum_a \\pi(s,a)Q(s,a)$\n",
        "\n",
        "Estos ajustes deberian ser realizadas hasta converger, pero por ahora solo los repetiremos 50 veces.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "xeLUhNuK3oT5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "V = np.zeros(nb_states)\n",
        "Q = np.zeros((nb_states, nb_actions))\n",
        "\n",
        "V_high_history = []\n",
        "V_low_history = []\n",
        "\n",
        "for k in range(50):\n",
        "\n",
        "    Q[s['alto'], a['buscar']] = 0 #TODO\n",
        "    Q[s['alto'], a['esperar']] = 0 #TODO\n",
        "\n",
        "    Q[s['bajo'], a['buscar']] = 0 #TODO\n",
        "    Q[s['bajo'], a['esperar']] = 0 #TODO\n",
        "    Q[s['bajo'], a['recargar']] = 0 #TODO\n",
        "\n",
        "    V[s['alto']] = #TODO\n",
        "    V[s['bajo']] = #TODO\n",
        "\n",
        "    V_alto_history.append(V[s['alto']])\n",
        "    V_bajo_history.append(V[s['bajo']])\n",
        "\n",
        "print(Q)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(V_alto_history, label=\"alto\")\n",
        "plt.plot(V_bajo_history, label=\"bajo\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "vsl_wLP35CxC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Pregunta 2:** ¿Convergen los valores Q? ¿Qué tan rápido? Cambie los valores de $\\gamma$ y concluya sobre su importancia (no se olvide de reiniciar los valores de V y Q a 0).\n",
        "\n",
        "**Pregunta 3:** Imprima los valores de Q al final del proceso de evaluación. ¿Cómo sería una estrategia golosa con respecto a estos valores Q?\n",
        "\n",
        "**Pregunta 4:** Cambie la estrategia a la obtenida en la pregunta anterior y evalue. ¿Que ocurre? Compare los valores finales. ¿Cual es mejor?\n",
        "\n"
      ],
      "metadata": {
        "id": "a2zueGgc7MAA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Iteración de estrategias\n",
        "\n",
        "Para mejorar una estrategia podemos mirar los valores Q en cada- estado y luego cambiar la estrategia para que escoga la acción con mayor Q. Si esto no cambia la estrategia (escogemos la misma acción) entonces encontramos la estrategia óptima y podemos deternos.\n",
        "\n",
        "**Pregunta 5:** Implemente la iteración de estrategias.\n",
        "\n",
        "No se olvide de reiniciar los arreglos V y Q y la estrategia\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Dh5lssFa8bs3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "V = np.zeros(nb_states)\n",
        "Q = np.zeros((nb_states, nb_actions))\n",
        "\n",
        "pi = np.array([a['buscar'], a['buscar']], dtype=int)\n",
        "\n",
        "V_alto_history = []\n",
        "V_bajo_history = []\n",
        "\n",
        "t = 1\n",
        "while True:\n",
        "    # Policy evaluation\n",
        "    for k in range(50):\n",
        "\n",
        "        Q[s['alto'], a['buscar']] = 0 #TODO\n",
        "        Q[s['alto'], a['esperar']] = 0 #TODO\n",
        "\n",
        "        Q[s['bajo'], a['buscar']] = 0 #TODO\n",
        "        Q[s['bajo'], a['esperar']] = 0 #TODO\n",
        "        Q[s['bajo'], a['recargar']] = 0 #TODO\n",
        "\n",
        "        V[s['alto']] = 0 #TODO\n",
        "        V[s['bajo']] = 0 #TODO\n",
        "\n",
        "        V_alto_history.append(V[s['alto']])\n",
        "        V_bajo_history.append(V[s['bajo']])\n",
        "\n",
        "    # Mejora de la estrategia\n",
        "    pi_old = pi.copy()\n",
        "    pi[s['alto']] = 0 #TODO\n",
        "    pi[s['bajo']] = 0 #TODO\n",
        "\n",
        "    print('Estrategia golosa después de la iteracion ', t)\n",
        "    print('pi(alto)=', pi[s['alto']])\n",
        "    print('pi(bajo)=', pi[s['bajo']])\n",
        "    print('-')\n",
        "\n",
        "    # Terminar si la estrategia ya no cambia\n",
        "    if pi[s['alto']] == pi_old[s['alto']] and pi[s['bajo']] == pi_old[s['bajo']]:\n",
        "        break\n",
        "    t += 1\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(V_alto_history, label=\"alto\")\n",
        "plt.plot(V_bajo_history, label=\"bajo\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "UZ2lupDa9d7i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Iteración de Valores\n",
        "\n",
        "En la iteración de valores, unimos la evaluación de estrategias y la mejora en una sola regla:\n",
        "\n",
        "$V(s) = \\max_a \\sum_s p(s'|s,a)[r(s,a,s')+\\gamma V^\\pi(s')]$\n",
        "\n",
        "**Pregunta 6:** Modifique el código anterior para implementar iteración de valores.. Use una cantidad fija de iteraciones (50). Muestre la evolución de los valores de V e imprima la estrategia golosa después de cada iteración. Concluya.\n"
      ],
      "metadata": {
        "id": "bbZ7U9XX-5JQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "V = np.zeros(nb_states)\n",
        "Q = np.zeros((nb_states, nb_actions))\n",
        "\n",
        "pi = np.array([a['buscar'], a['buscar']], dtype=int)\n",
        "\n",
        "V_alto_history = []\n",
        "V_bajo_history = []\n",
        "\n",
        "for k in range(50):\n",
        "\n",
        "    # Policy evaluation\n",
        "    Q[s['alto'], a['buscar']] = 0 #TODO\n",
        "    Q[s['alto'], a['esperar']] = 0 #TODO\n",
        "\n",
        "    Q[s['bajo'], a['buscar']] = 0 #TODO\n",
        "    Q[s['bajo'], a['esperar']] = 0 #TODO\n",
        "    Q[s['bajo'], a['recargar']] = 0 #TODO\n",
        "\n",
        "    V[s['alto']] = 0 #TODO\n",
        "    V[s['bajo']] = 0 #TODO\n",
        "\n",
        "    V_alto_history.append(V[s['alto']])\n",
        "    V_bajo_history.append(V[s['bajo']])\n",
        "\n",
        "    # Compute the greedy policy\n",
        "    pi_old = pi.copy()\n",
        "    pi[s['alto']] = 0 #TODO\n",
        "    pi[s['bajo']] = 0 #TODO\n",
        "\n",
        "print('Greedy policy after iteration', k)\n",
        "print('pi(high)=', pi[s['alto']])\n",
        "print('pi(low)=', pi[s['bajo']])\n",
        "print(\"V=\", V)\n",
        "print(\"Q=\", Q)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(V_high_history, label=\"alto\")\n",
        "plt.plot(V_low_history, label=\"bajo\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "gYWkOUeWAiC9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Pregunta 7:** Cambie el valor de $\\gamma =0.3$ para que el agente valore mas las recompensas recientes. ¿Cambia la estrategia?"
      ],
      "metadata": {
        "id": "a6mGucrdBnDK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Pregunta 8:** Cambie $\\gamma$ a 0.99 ¿Que cambia y por qué?\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "N98P_hcrB3sC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Escriba su codigo aqui"
      ],
      "metadata": {
        "id": "rlHD1z5eCzJN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Pregunta 9:** Cambie los parametros a:\n",
        "\n",
        "$\\alpha = 0.01$\n",
        "$\\beta = 0.2$\n",
        "$\\gamma = 0.7$\n",
        "$\\mathcal{R}^\\text{buscar} = 6$\n",
        "$\\mathcal{R}^\\text{esperar} = 5$\n"
      ],
      "metadata": {
        "id": "N1TDBT3TCxLH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Escriba su codigo aqui"
      ],
      "metadata": {
        "id": "aQcdNMS8D1k1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Pregunta 10:** Encuentre un conjunto de parametros donde sea optima la acción buscar estando en el estado bajo."
      ],
      "metadata": {
        "id": "r1H3h9ZLFcYv"
      }
    }
  ]
}