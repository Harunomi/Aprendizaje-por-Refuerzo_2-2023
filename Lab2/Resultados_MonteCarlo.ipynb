{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementación del agente con Monte Carlo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "rng = np.random.default_rng()\n",
    "\n",
    "class MonteCarloAgent:\n",
    "    '''\n",
    "    Monte-Carlo agent.\n",
    "    '''\n",
    "\n",
    "    def __init__(self, env, gamma, epsilon, alpha):\n",
    "        '''\n",
    "        Constructor.\n",
    "\n",
    "        @param env: the environment\n",
    "        @param gamma: the discount factor\n",
    "        @param epsilon: the probability to explore\n",
    "        @param alpha: the learning rate\n",
    "        '''\n",
    "        self.env = env\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.alpha = alpha\n",
    "\n",
    "\n",
    "        # Q_table\n",
    "        self.Q = {}\n",
    "        \n",
    "\n",
    "    def act(self, state):\n",
    "        # Convertir la lista a una tupla usando hash para hacerla hashable\n",
    "        state_key = hash(str(state))\n",
    "        \n",
    "        # Acceder al valor Q correspondiente al estado\n",
    "        q_values = self.Q.get(state_key, np.zeros(self.env.action_space.n))\n",
    "        action = np.argmax(q_values)\n",
    "\n",
    "        if rng.uniform() < self.epsilon:\n",
    "            action = rng.choice(self.env.action_space.n)\n",
    "        \n",
    "        action_to_names = {\n",
    "            0: 'RIGHT',\n",
    "            1: 'DOWN',\n",
    "            2: 'LEFT',\n",
    "            3: 'UP',\n",
    "            4: 'BOMB',\n",
    "            5: 'WAIT',\n",
    "        }\n",
    "\n",
    "        #print(\"Action: \", action_to_names[action])\n",
    "        \n",
    "        return action\n",
    "    \n",
    "    def update(self, episode):\n",
    "\n",
    "        return_episode = 0.0\n",
    "\n",
    "        # iterate backwards over the episode\n",
    "\n",
    "        for state, action, reward in reversed(episode):\n",
    "            \n",
    "            state_key = hash(str(state))\n",
    "\n",
    "\n",
    "            return_episode = reward + self.gamma * return_episode\n",
    "            q_values = self.Q.get(state_key, np.zeros(self.env.action_space.n))\n",
    "            q_values[action] = q_values[action] + self.alpha * (return_episode - q_values[action])\n",
    "            self.Q[state_key] = q_values\n",
    "\n",
    "\n",
    "    def train(self, nb_episode, recorder=False):\n",
    "        ''''\n",
    "        Runs the agent on the environment\n",
    "        '''\n",
    "        returns = []\n",
    "        steps = []\n",
    "\n",
    "        for episode in range(nb_episode):\n",
    "            print(\"Episodio nº: \", episode)\n",
    "            # reset\n",
    "            state, info = self.env.reset()\n",
    "            return_episode = 0.0\n",
    "            nb_steps = 0\n",
    "            done = False\n",
    "\n",
    "            transition = []\n",
    "\n",
    "            while not done:\n",
    "                # act\n",
    "                action = self.act(state)\n",
    "\n",
    "                # step\n",
    "                next_state, reward,terminal, truncated, info = self.env.step(action)\n",
    "\n",
    "                # update\n",
    "                transition.append((state, action, reward))\n",
    "\n",
    "                state = next_state\n",
    "\n",
    "                done = terminal or truncated\n",
    "                nb_steps += 1\n",
    "\n",
    "                return_episode += reward \n",
    "\n",
    "            self.update(transition)\n",
    "\n",
    "            # Store info\n",
    "            returns.append(return_episode)\n",
    "            steps.append(nb_steps)\n",
    "        \n",
    "        return returns,steps"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resultados obtenidos con Monte Carlo"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recompensa\n",
    "En esta entrega, los valores para las recompensas se plantearon de la siguiente manera:\n",
    "* Colocar una bomba: +10pts\n",
    "* Romper una caja: +7pts\n",
    "* Eliminar un enemigo: +5pts\n",
    "* Llegar al objetivo: +100pts\n",
    "* Morir por una explosión: -100pts\n",
    "* Morir por un enemigo: -100pts\n",
    "* Vagar luego de destruir la caja donde se encuentra el objetivo: -5pts\n",
    "\n",
    "Además para poder entrenar al agente probamos diferentes escenarios, variando las dimensiones de la grilla, la cantidad de cajas rompibles y la cantidad de enemigos. Además experimentamos con diferentes valores para las variables gamma, epsilon y alfa\n",
    "\n",
    "En base a lo anteriormente mencionado, se obtuvieron los siguientes resultados:\n",
    "- Dimensiones de la grilla: 5x5\n",
    "- Cantidad cajas rompibles: 1\n",
    "- Cantidad de enemigos: 1 vertical\n",
    "- Gamma: 0,9\n",
    "- Epsilon: 0,5\n",
    "- Alfa: 0,5\n",
    "\n",
    "<div><img src=\"https://i.ibb.co/09G1hMz/Captura-de-Pantalla-2023-11-20-a-la-s-23-29-38.png\" width=550/></div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Dimensiones de la grilla: 5x5\n",
    "- Cantidad cajas rompibles: 1\n",
    "- Cantidad de enemigos: 1 vertical\n",
    "- Gamma: 0,9\n",
    "- Epsilon: 0,9\n",
    "- Alfa: 0,5\n",
    "\n",
    "<div><img src=\"https://i.ibb.co/3Sgs0xM/Captura-de-Pantalla-2023-11-20-a-la-s-23-34-44.png\" width=550/></div>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Después de experimentar con diferenetes entradas se llega a ciertas conclusiones con respecto a los valores que pueden tomar gamma, epsilon y alfa. Los valores eleguidos son:\n",
    "- Gamma: 1,0\n",
    "- Epsilon: 0,1\n",
    "- Alfa: 0,4\n",
    "\n",
    "### El agente al tener estos valores tiene las siguientes carácteristicas: al tener un gran valor de gamma, tenemos que el agente valora tanto la recompensa actual como la recompensa futura, un valor bajo de epsilon nos dice que tenemos un agente más conservador, el cual, explota más de lo que explora y al tener un valor equilibrado de alfa tenemos que el ajuste de las estimaciones no serán ni muy rápidas ni muy lentas.\n",
    "\n",
    "### Aquí podemos ver un ejemplo con las siguientes carácteristicas:\n",
    "- Dimensiones de la grilla: 8x8\n",
    "- Cantidad cajas rompibles: 6\n",
    "- Cantidad de enemigos: 5 vertical, 5 horizontal\n",
    "- Gamma: 1,0\n",
    "- Epsilon: 0,1\n",
    "- Alfa: 0,4\n",
    "\n",
    "<div><img src=\"https://i.ibb.co/qjt7n4G/Captura-de-Pantalla-2023-11-21-a-la-s-02-43-49.png\" width=550/></div>\n",
    "\n",
    "### Con esto podemos ver que tenemos más recompensas positivas, sin embargo, todavía tenemos recompensas negativas altas. Es por esto que a partir de estos valores, hay que empezar a ver las limitantes: dimensiones de la grilla, cantidad de cajas y cantidad de enemigos.\n",
    "### Siguiendo con el ejemplo anterior, pero cambiando la cantidad de cajas indestructibles a 16 fijas, tenemos lo siguiente:\n",
    "<div><img src=\"https://i.ibb.co/BZ4bvp7/Captura-de-Pantalla-2023-11-21-a-la-s-05-22-32.png\" width=550/></div>\n",
    "\n",
    "\n",
    "### De esto podemos concluir que la cantidad de cajas destructibles tiene que ser el doble del valor de la dimensión de la grilla, en este caso 2 x 8 = 16. Así que por convención se tomará este criterio para la cantidad de cajas.\n",
    "### Ahora es necesario definir la cantidad de enemigos. Después de varias pruebas y manteniendo los parámetros anteriores, pero variando la cantidad de enemigos siempre con la misma cantidad de enemigos con movimiento horizontal y movimiento vertical, en este caso son 7 y 7, se obtuvieron los siguientes resultados:\n",
    "\n",
    " <div><img src=\"https://i.ibb.co/bHsCNNH/Captura-de-Pantalla-2023-11-21-a-la-s-03-46-53.png\" width=550/></div>\n",
    "\n",
    " ### En este caso es aceptable tomar estos 7 enemigos verticales y 7 enemigos horizontales, pero este sería el límite y se va a definir como regla tomar una cantidad menor a las dimensiones de la grilla. Cuando se toma una mayor cantidad de enemigos, no se obtienen malos resultados, pero las barras del returns que es donde se ven reflejadas las recompensas, parten en valores menores a 0.\n",
    "\n",
    " ### Por último es necesario definir las dimensiones máximas. \n",
    " ### Para esto se experimento con varias dimensiones pero finalmente, se determinó que las dimensiones más acertadas eran 15x15, con 30 cajas fijas, 14 enemigos verticlaes y 14 enemigos horizontales. Con estos valores la ejecución se realentiza bastante, pero todavía tiene una velocidad prudente y los resultados son los siguientes:\n",
    "\n",
    " <div><img src=\"https://i.ibb.co/svWHmVb/Captura-de-Pantalla-2023-11-21-a-la-s-05-52-41.png\" width=550/></div>\n",
    "\n",
    "  "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
